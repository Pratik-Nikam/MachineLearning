{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIRptFoEm22Wmg4vyce79S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratik-Nikam/MachineLearning/blob/main/HDFC_FAQ_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8FXF7Y-klky"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# Check GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data\n",
        "with open('/content/sample_data/HDFC_Faq.txt', 'r') as f:\n",
        "    data = json.load(f)\n",
        "questions = [entry['question'] for entry in data]\n",
        "answers = [entry['answer'] for entry in data]\n",
        "\n",
        "# Split data\n",
        "train_questions, temp_questions, train_answers, temp_answers = train_test_split(\n",
        "    questions, answers, test_size=0.2, random_state=42\n",
        ")\n",
        "val_questions, test_questions, val_answers, test_answers = train_test_split(\n",
        "    temp_questions, temp_answers, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Determine optimal max_length\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "tokenized_lengths = [len(tokenizer.encode(q)) for q in questions] + [len(tokenizer.encode(a)) for a in answers]\n",
        "max_length = min(max(tokenized_lengths) + 10, 512)\n",
        "print(f\"Using max_length: {max_length}\")\n",
        "\n",
        "# Tokenize\n",
        "train_encodings = tokenizer(train_questions, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "train_labels_encodings = tokenizer(train_answers, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "val_encodings = tokenizer(val_questions, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "val_labels_encodings = tokenizer(val_answers, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "# Prepare decoder inputs and labels\n",
        "train_decoder_input_ids = train_labels_encodings[\"input_ids\"][:, :-1]\n",
        "train_labels = train_labels_encodings[\"input_ids\"][:, 1:]\n",
        "val_decoder_input_ids = val_labels_encodings[\"input_ids\"][:, :-1]\n",
        "val_labels = val_labels_encodings[\"input_ids\"][:, 1:]\n",
        "train_decoder_attention_mask = train_labels_encodings[\"attention_mask\"][:, :-1]\n",
        "val_decoder_attention_mask = val_labels_encodings[\"attention_mask\"][:, :-1]\n",
        "\n",
        "# Create datasets\n",
        "batch_size = 8\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        \"input_ids\": train_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": train_encodings[\"attention_mask\"],\n",
        "        \"decoder_input_ids\": train_decoder_input_ids,\n",
        "        \"decoder_attention_mask\": train_decoder_attention_mask\n",
        "    },\n",
        "    train_labels\n",
        ")).batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        \"input_ids\": val_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": val_encodings[\"attention_mask\"],\n",
        "        \"decoder_input_ids\": val_decoder_input_ids,\n",
        "        \"decoder_attention_mask\": val_decoder_attention_mask\n",
        "    },\n",
        "    val_labels\n",
        ")).batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "DWn0UHQUo2Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and train model\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5))\n",
        "\n",
        "start_time = time.time()\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=1, verbose=1)\n",
        "print(f\"Training time: {(time.time() - start_time) / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "id": "jn0tI2fto7HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"/content/fine_tuned_t5_model\")\n",
        "\n",
        "# Generate answer\n",
        "def generate_answer(question, model, tokenizer):\n",
        "    inputs = tokenizer(question, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=100, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "loaded_model = TFT5ForConditionalGeneration.from_pretrained(\"/content/fine_tuned_t5_model\")\n",
        "print(f\"Predicted Answer: {generate_answer('How do I change my password?', loaded_model, tokenizer)}\")"
      ],
      "metadata": {
        "id": "RjKWSWAVo9KM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}